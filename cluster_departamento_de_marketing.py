# -*- coding: utf-8 -*-
"""cluster  Departamento de Marketing

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ckwD-E8h4Dcg94i3PKmp-Pzk0NdFcMfM

# Departamento de marketing

- Dados: https://www.kaggle.com/arjunbhasin2013/ccdata

para está análise usaremos os dados disponível no link acima

## Importação das bibliotecas e base de dados
"""

import pandas as pd
import numpy as np #funções matemáticas 
import seaborn as sns #gerações de gráficos 
import matplotlib.pyplot as plt #gerações de gráficos 
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans #algoritimo para segmentação do mercado 
from sklearn.decomposition import PCA  # utilizado para redução de dimensionalidade

"""Para facilitar a leitura dos dados  fiz o upload do aarquivo csv para o guithub, vamos carregar os dados atráves do comando abaixo."""

url = "https://raw.githubusercontent.com/JosefersonBarreto/analise_de_clusters_Departamento_de_marketing/main/Marketing_data.csv"



creditcard_df = pd.read_csv(url)

creditcard_df.shape

"""nos temos 8950,cada linha indica um cliente do banco e temos 18 caractéristicas de cada cliente,vamos observar os primeiros 10 registros"""

creditcard_df.head(2)

"""aqui podemos ver as nossa variáveis discritas anteriormente 


**CUST_ID** : Identificação do Titular do Cartão de Crédito (Categórico)

**BALANCE**: Saldo que resta na conta para fazer compras 

**BALANCE_FREQUENCY** : Com que frequência o Saldo é atualizado, pontuação entre 0 e 1 (1 = atualizado com frequência, 0 = não atualizado com frequência)
COMPRAS : Quantidade de compras feitas na conta

**ONEOFF_PURCHASES** : Valor máximo de compra feito de uma só vez

**INSTALLMENTS_PURCHASES** : Valor de compra feito em parcela

**CASH_ADVANCE** : Dinheiro adiantado fornecido pelo usuário

**PURCHASES_FREQUENCY** : Com que frequência as compras estão sendo feitas, pontuação entre 0 e 1 (1 = comprado com frequência, 0 = não comprado com frequência)

**ONEOFFPURCHASESFREQUENCY**: Com que frequência as compras estão acontecendo de uma só vez (1 = compradas com frequência, 0 = não compradas com frequência)

**PURCHASESINSTALLMENTSFREQUENCY** : com que frequência as compras parceladas estão sendo feitas (1 = feitas com frequência, 0 = não feitas com frequência)

**CASHADVANCEFREQUENCY** : com que frequência as dinheiro adiantado sendo pago

**CASHADVANCETRX** : Número de transações feitas com "Cash in Advanced"

**PURCHASES_TRX** : Número de transações de compra feitas

**CREDIT_LIMIT** : Limite de cartão de crédito para o usuário

**PAYMENTS** : Valor do pagamento feito pelo usuário

**MINIMUM_PAYMENTS** : Valor mínimo dos pagamentos feitos pelo usuário

**PRCFULLPAYMENT**: Porcentagem do pagamento integral pago pelo usuário

**POSSIBILIDADE** : Posse do serviço de cartão de crédito para o usuário

"""

creditcard_df.info()

"""podemos perceber que  temos dados faltantes em nosso conjunto de dados

# Fazendo a Descrição dos Nossos Dados
"""

creditcard_df.describe()

"""aqui temos algumas informações importantes,como média,desvio padrão,mediana,entre outras informações, por exemplo,  a média de saldo na conta corrente(**BALANCE**) é de 1564 apróximadamente, já o cliente que apresenta o menor valor na conta correte é de 0 e valor mmáximo é de 19.043 doláres ,além disso temos o valor médio das compras(**PURCHES**) que é de 1003 dólares apróximadamente,entre outras informações que podiamos extrair. """

#creditcard_df[creditcard_df['ONEOFF_PURCHASES'] == 40761.250000] função de filtragem

"""vendo o cliente que diantou o maior número de crédito """

creditcard_df[creditcard_df['CASH_ADVANCE'] == creditcard_df['CASH_ADVANCE'].max()]

"""## <font  color=white face=arial black size=9 > Visualização e exploração dos dados </font>

Vamos utilizar o heatmap para verificar se temos registros nulos
"""

sns.heatmap(creditcard_df.isnull());

"""podemos perceber que  pelo hitmap a variável **MINIMUM_PAYMENTS** tem valores nulos , além dissso, pela função **info** a variável CREDIT_LIMIT  também apresentou valores ausentes , outra forma de vermos isso é atráves do código abaixo."""

creditcard_df.isnull().sum()

"""Vamos fazer a imputação dos valores ausentes, para isso iremos substituir os valores aausentes pelo valor mediano da coluna em questão, devido ao fato que a média ser muito sensível a valores extremos,por isso optamos por utilizar a mediana nesse caso """

import statistics as sta
#creditcard_df['MINIMUM_PAYMENTS'].sta.median(creditcard_df['MINIMUM_PAYMENTS'])
sta.median(creditcard_df['MINIMUM_PAYMENTS'])

#creditcard_df['MINIMUM_PAYMENTS'].median()

"""substituindo os valores ausentes na variável **MINIMUM_PAYMENTS** pela mediana 



"""

creditcard_df.loc[(creditcard_df['MINIMUM_PAYMENTS'].isnull() == True), 'MINIMUM_PAYMENTS'] = sta.median(creditcard_df['MINIMUM_PAYMENTS'])

"""Fazendo o mesmo procedimento para variável **CREDIT_LIMIT**




"""

creditcard_df.loc[(creditcard_df['CREDIT_LIMIT'].isnull() == True), 'CREDIT_LIMIT'] = sta.median(creditcard_df['CREDIT_LIMIT'])

"""Vamos verificar se ainda temos valores nulos presentes em nosso conjunto de dados"""

creditcard_df.isnull().sum()

sns.heatmap(creditcard_df.isnull());

"""Como podemos ver,não temos mais a presença de valores ausentes em nosso dataset,podemos prosseguir para próxima etapa.

## Verificando se temos colunas duplicadas
"""

creditcard_df.duplicated().sum()

"""como a coluna **CUT_ID** é apenas pra identificação dos clientes,vamos remover está coluna, pois não será importante para o modelo """

creditcard_df.drop('CUST_ID', axis = 1, inplace = True)

creditcard_df.head()

creditcard_df.columns

len(creditcard_df.columns)

"""Ficamos com 17 colunas ,como nossas variaveis são do tipo númericas,vamos mostrar os gráficos das distribuição dos dados 

"""

plt.figure(figsize=(10,50))
for i in range(len(creditcard_df.columns)):
  plt.subplot(17, 1, i + 1)
  sns.distplot(creditcard_df[creditcard_df.columns[i]], kde = True)
  plt.title(creditcard_df.columns[i])
plt.tight_layout();

"""# Verificando a correlação"""

correlations = creditcard_df.corr()

f, ax = plt.subplots(figsize=(20,20))
sns.heatmap(correlations, annot=True);

"""Podemos perceber que em alguns casos temos baixa correlação entre nossas  variáveis,mas temos casos onde  as variáveis  preditoras apresentam uma forte correlação de 0.8  que ocontece com C**ASH_ADVANCE_FREQUENCY e ASH_ADVANCE_TRX**

<font size=9 > Implementação do K-means  </font>

Este é um algoritmo de apredizagem não supervisionada ,nele os registros são agrupados baseados em atributos similares,por meio do cálculo da distância euclidiana.

## Definição do número de clusters usando o Elbow Method

- Mais detalhes 
  - https://en.wikipedia.org/wiki/Elbow_method_(clustering)
  - https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/

Vamos verificar os valores mínimo e máximo para a primeira variável(**BALANCE**)
"""

min(creditcard_df['BALANCE']), max(creditcard_df['BALANCE'])

"""Como podemos perceber, existe uma grande diferença entre o valor mínimo e máximo,ou seja, se rodarmos o modelo nessa forma , provavelmente ele iria considerar um atributo mais importante que outro,logo é importante colocar os dados na mesma escala.

<font size=9 > Padronizando   </font>
"""

scaler = StandardScaler()
creditcard_df_scaled = scaler.fit_transform(creditcard_df)

"""nossos dados agora estão em formato numpy """

type(creditcard_df_scaled), type(creditcard_df)

min(creditcard_df_scaled[0]), max(creditcard_df_scaled[0])

"""podemos perceber que de fato o intervalo entre o menor e o maior valor diminuiu """

creditcard_df_scaled

"""Vamos definir o número de clusters inicial como 20,depois fazer o gráfico e utilizar a regra do cotovelo"""

wcss_1 = []
range_values = range(1, 20)
for i in range_values:
  kmeans = KMeans(n_clusters=i,random_state=0)
  kmeans.fit(creditcard_df_scaled)
  wcss_1.append(kmeans.inertia_)

print(wcss_1)

plt.plot(wcss_1, 'bx-')
plt.xlabel('Clusters')
plt.ylabel('WCSS');

"""Pode-se perceber que entre o cluster 7 e 8 temos um início de um comportamento linear,então pela regra do cutuvelo vamos utilizar o k = 8

## Agrupamento com k-means

Vamos utilizar o número de clusters inicial como 8, o argumento **random_state=0** permitirá que obteremos os mesmos valores de clusters toda vez que rodarmos o código abaixo.
"""

kmeans = KMeans(n_clusters=8,random_state=0)
kmeans.fit(creditcard_df_scaled)

labels = kmeans.labels_

labels, len(labels)

np.unique(labels, return_counts=True)

kmeans.cluster_centers_

cluster_centers = pd.DataFrame(data = kmeans.cluster_centers_, columns = [creditcard_df.columns])
cluster_centers

"""- Grupo 4 (VIP/Prime): limite do cartão alto (15570) e o mais alto percentual de pagamento da fatura completa (0.48). Aumentar o limite do cartão e o hábito de compras

- Grupo 6: Clientes que pagam poucos juros para o banco e são cuidadosos com seu dinheiro. Possui menos dinheiro na conta corrente (105 dólares) e não sacam muito dinheiro do limite do cartão(**CASH_ADVANCE**)  303 dólares. Apenas 24% apróximadamente desses clientes pagam a fatura completa(**PRC_FULL_PAYMENT**).

- Grupo 3: usam o cartão de crédito como "empréstimo" (setor mais lucrativo para o banco), possuem muito dinheiro na conta corrente(**BALANCE**) 5067 dólares e sacam muito dinheiro do cartão de crédito (5207), compram pouco(**PURCHASES_FREQUENCY**) cerca de 30%  e usam bastante o limite do cartão para saques(**CASH_ADVANCE_FREQUENCY**) 52% apróximadamente.Além disso pagam muito pouco a fatura completa 4%,apróximadamente,logo, é um grupo de risco para aumentar o limite do cartão.

- Grupo 2 (clientes novos): clientes mais novos (7.23) e que possui menor limite de crédito 2427 dólares.



"""

cluster_centers = scaler.inverse_transform(cluster_centers)
cluster_centers = pd.DataFrame(data = cluster_centers, columns = [creditcard_df.columns])
cluster_centers

labels, len(labels)

"""Adicionando cada um dos grupos a cada um dos clientes """

creditcard_df_cluster = pd.concat([creditcard_df, pd.DataFrame({'cluster': labels})], axis = 1)
creditcard_df_cluster.head()

"""Gerando hsitogramaspara cada atributo e cada clusters """

for i in creditcard_df.columns:
  plt.figure(figsize=(35,5))
  for j in range(8):
    plt.subplot(1, 8, j + 1)
    cluster = creditcard_df_cluster[creditcard_df_cluster['cluster'] == j]
    cluster[i].hist(bins = 20)
    plt.title('{} \nCluster {}'.format(i, j))
  plt.show()

"""Ordenando os dados pela ordem de clusters """

credit_ordered = creditcard_df_cluster.sort_values(by = 'cluster')
credit_ordered.head()

"""Vendo os últimos 5 registros """

credit_ordered.tail()

"""Salvando os resultados em uma nova base de dados csv"""

credit_ordered.to_csv('cluster.csv')

"""## Aplicação de PCA (principal component analysis) e visualização dos resultados

É utilizado para a redução de dimencionalidade,onde tenta manter as informações originais com as mesmas características,onde será encontrado um novo conjunto de características chamadas de componentes, esses componentes são criados por meio de características não correlacionadas.
"""

pca = PCA(n_components=2)
principal_comp = pca.fit_transform(creditcard_df_scaled)
principal_comp

pca_df = pd.DataFrame(data = principal_comp, columns=['pca1', 'pca2'])
pca_df.head()

pca_df = pd.concat([pca_df, pd.DataFrame({'cluster': labels})], axis = 1)
pca_df.head()

plt.figure(figsize=(10,10))
sns.scatterplot(x = 'pca1', y = 'pca2', hue = 'cluster', data = pca_df, palette = ['red', 'green', 'blue', 'pink', 'yellow', 'gray', 'purple', 'black'])

"""<font size= 9 > Utilizando autoencoders </font>

Autoencoders  são redes neurais artificiais para  codificar dados,onde será utilizada a mesma entrada e a mesma saída para comparar os resultados,vamos utilizar essa técnica para redução de dimensionalidade

## Aplicação de autoencoders
"""

# 18 -> 10 iremos fazer a redução de dimensionalidade  de 18 para 10 ,com base na correlação dos atributos 
# aplicaremos o método Elbow
# executaremos novamente o K-means
# e por último faremos a análise de componentes principais(PCA)

creditcard_df_scaled.shape

"""como nossos dados não são do tipo imagem , primeiro aumentaremos o número de  camadas de 17 para 500 e depois para 2000,depois deminuiremos para 10 que são as camadas centrais e faremos o processo inverso, de 10 vamos pra 2000, de 2000 vamos para 500  e de  500 vamos para 17. Essa é uma tecnica muito utilizada quando temos um conjunto de dados pequeno 

"""

# 17 -> 500 -> 2000 -> 10 -> 2000 -> 500 -> 17
from tensorflow.keras.layers import Input, Dense # camada de entrada, onde recebe os 17 atributos 
from tensorflow.keras.models import Model #camada de contrução da rede neural

"""implementando usando a função de ativação **relu**"""

import tensorflow as tf



# Fazendo o  procedimento citado acima 
# 17 -> 500 -> 2000 -> 10 -> 2000 -> 500 -> 17
np.random.seed(42)
tf.random.set_seed(42)

input_df = Input(shape=(17,)) #atributos base

# 17 -> 500
x = Dense(500, activation='relu')(input_df)


# 500 -> 2000 
x = Dense(2000, activation='relu')(x)

# 2000 -> 10 
encoded = Dense(10, activation='relu')(x)

#10 -> 2000 
x = Dense(2000, activation='relu')(encoded)

#2000 -> 500
x = Dense(500, activation='relu')(x)

#500 -> 17
decoded = Dense(17)(x)

"""Criando o autoencoder passando a camanda incial **input_df**, e a camada final **decoded**"""

# autoencoder
autoencoder = Model(input_df, decoded)

"""Criando o encoder que guardará as informações da camada central,para isso passamos a camada incial **input_df** e a camada cenetral **encoded**"""

# encoder
encoder = Model(input_df, encoded)

"""Fazendo a copilação do modelo utilizando o optimizador **Adam** para ajustes dos pesos e afunção de erro que será o erro ao quadrado **mean_Squared_error**"""

autoencoder.compile(optimizer = 'Adam', loss = 'mean_squared_error')

"""Como citamos acima,sera comparado os dados de entrada com os próprios dados de entrada,o núremo de epócas será igual a 50 """

autoencoder.fit(creditcard_df_scaled, creditcard_df_scaled, epochs = 50)

"""vendo o tamanho """

creditcard_df_scaled.shape

compact = encoder.predict(creditcard_df_scaled)

compact.shape

"""tamanho dos dados originais """

creditcard_df_scaled[0]

"""tamanho com os dados compactados """

compact[0]

"""aplicando o método Elbow nos novos dados(conjunto comapctado anteriormente)"""

wcss_2 = []
range_values = range(1, 20)
for i in range_values:
  kmeans = KMeans(n_clusters=i,random_state=0)
  kmeans.fit(compact)
  wcss_2.append(kmeans.inertia_)

plt.plot(wcss_2, 'bx-')
plt.xlabel('Clusters')
plt.ylabel('WCSS');

plt.plot(wcss_1, 'bx-', color = 'r')
plt.plot(wcss_2, 'bx-', color = 'g');

"""podemos ver que entre 3 e 4 a linha verde começa a ter um comportamento linear, então pela regra do cotovelo vamos utilizar o k = 4 """

kmeans = KMeans(n_clusters=4,random_state=0)
kmeans.fit(compact)
labels = kmeans.labels_

"""Salvando os labels do clusters """

labels = kmeans.labels_
labels, labels.shape

"""Fazendo a concatenação com a base original """

df_cluster_at = pd.concat([creditcard_df, pd.DataFrame({'cluster': labels})], axis = 1)
df_cluster_at.head()

#for i in creditcard_df.columns:
 # plt.figure(figsize=(35,5))
#  for j in range(4):
   # plt.subplot(1, 4, j + 1)
   # cluster = df_cluster_at[df_cluster_at['cluster'] == j]
   # cluster[i].hist(bins = 20)
    #plt.title('{} \nCluster {}'.format(i, j))
 # plt.show()

"""# Vamos separar o cluster para poder extrair informações importantes e poder interpretar melho os resultados """

grupo0= df_cluster_at[df_cluster_at['cluster'] == 0]

grupo0.describe()


# grupo formado por novos clientes , que possuem pouco saldo em conta (**BALANCE**) apenas 1252 dólares e que compram menos em relação ao grupo 2 que é o mais cauteloso

grupo1= df_cluster_at[df_cluster_at['cluster'] == 1]

grupo1.describe()

# grupo com bom saldo em conta, que mais utiliza o saldo do cartão **CASH_ADVANCE_FREQUENCY** , porém que paga apenas 9% da parcela total,sendo considerado um grupo de risco

grupo2= df_cluster_at[df_cluster_at['cluster'] == 2]

grupo2.describe()


# grupo com menor saldo em conta, que são mais caltelos 
#grupo com menor  cash_advanced e que sacam dinheiro do cartão de crédito em apenas 6%  dos casos apróximadamente

grupo3= df_cluster_at[df_cluster_at['cluster'] == 3]

grupo3.describe()
#grupo com maior saldo na conta , e que não sacam muito dinheiro do cartão de credito ,possui em média um limit de credito de 12020 dólares,
#apenas 30% fazem o pagamento total da parcela

"""# Interpretação

- Grupo 3 (VIP/Prime): limite do cartão alto (12020) e o mais alto percentual de pagamento da fatura completa (0.30). Aumentar o limite do cartão e o hábito de compras

- Grupo 2:  grupo com menor saldo em conta, que são mais cautelososo 
possui  menor  **cash_advanced** 290 dólares  e que sacam dinheiro do cartão de crédito em apenas 6%  dos casos apróximadamente. 

- Grupo 1: usam o cartão de crédito como "empréstimo" (setor mais lucrativo para o banco), grupo com bom saldo em conta, que mais utiliza o saldo do cartão  48% das vezes(**CASH_ADVANCE_FREQUENCY**) , porém que paga apenas 9% da parcela total,sendo considerado um grupo de risco, 

- Grupo o (clientes novos): grupo formado por novos clientes , que possuem pouco saldo em conta (**BALANCE**) apenas 1252 dólares e que compram menos em relação ao grupo 2 que é o mais cauteloso

Fazendo a análise de componentes principais ,vamos dividir em dois grupos que seram as cordenadas dos pontos
"""

pca = PCA(n_components = 2)
prin_comp = pca.fit_transform(compact)
pca_df = pd.DataFrame(data = prin_comp, columns = ['pca1', 'pca2'])
pca_df.head()

"""Fazendo a concatenação dos coponentes encontrados com os clusters """

pca_df = pd.concat([pca_df, pd.DataFrame({'cluster': labels})], axis = 1)
pca_df.head()

plt.figure(figsize=(10,10))
sns.scatterplot(x = 'pca1', y = 'pca2', hue = 'cluster', data = pca_df, palette = ['red', 'green', 'blue', 'black'])

pca_df['cluster'] = pca_df['cluster'].astype('category')

import plotly.express as px
pca_df['size'] = np.random.randint(10, 100, size=len(pca_df))
fig = px.scatter(pca_df, x='pca1', y='pca2', color='cluster',title = 'Gráfico dos Clusters',color_discrete_sequence=['red' ,'orange','blue','purple'],size='size')

fig.update_layout(
    paper_bgcolor='#23262F',plot_bgcolor = "#23262F",title_x=0.5,title_font_color='white',font_color='white'
)
fig.show()

"""Aqui podemos ver uma visualização gráfica dos cluster  citados anteriormente , como foi citado ,aqui temos apenas uma representação gráfica.  Vamos ordenar a base de dados dos cluster para poder repassar ao setor de marketing para eles análisarem e poderem tomar decisões a quais grupos  investir  em campanhas publicitárias.

Ordenandos os clusters
"""

df_cluster_ordered = df_cluster_at.sort_values(by = 'cluster')
df_cluster_ordered.head()

df_cluster_ordered.tail()

df_cluster_ordered.to_excel('cluster_ordereded.xls');

"""# Conclusão

Como podemos ver, a análise de Cluster  é uma boa opção para tomarmos decisões com base em grupos, caso queiram reutilizar o código lembre-se que ao utilizar o método encoder para a redução de dimensionalidade outros valores podem ser gerados , sendo necessário  observar as caractéristicas dos cluster novamente devido a este fato .
"""